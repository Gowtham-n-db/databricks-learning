{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa61992f-7263-49f0-b739-1f0891d11986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading XML data with an inferred schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491a4149-9298-44cb-9db4-98f484eaab52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read JSON file into a DataFrame\n",
    "df = (spark.read.format(\"json\")\n",
    "      .option(\"multiLine\", \"true\")\n",
    "      .load(\"/Volumes/mycatalog/myschema/myvolume/repofiles/Data-Engineering-with-Databricks-Cookbook-main/data/nobel_prizes.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3491e4f2-968b-48ce-9aa4-beb932ae2aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d076324c-9ed6-4fdd-94ac-943e32014c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display contents of DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc46a4f-16e3-4a1f-8799-fa8993867bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e3cfd21-cccf-4a87-98f5-868b3b2db23a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355bafa5-363f-4a3a-96dd-e0594d212c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_flattened = (\n",
    "    df\n",
    "    .withColumn(\"laureates\",explode(col(\"laureates\"))) # Explode the laureates array column into rows\n",
    "    .select(col(\"category\")\n",
    "            , col(\"year\")\n",
    "            , col(\"overallMotivation\")\n",
    "            , col(\"laureates.id\")\n",
    "            , col(\"laureates.firstname\")\n",
    "            , col(\"laureates.surname\")\n",
    "            , col(\"laureates.share\")\n",
    "            , col(\"laureates.motivation\"))) # Use dot notion for columns in the STRUCT field\n",
    "\n",
    "df_flattened.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51943d8f-b7c2-490f-a5c8-105222ea8a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "json_schema = StructType(\n",
    "    [StructField('category', StringType(), True),\n",
    "     StructField('laureates', ArrayType(StructType(\n",
    "         [StructField('firstname', StringType(), True), \n",
    "          StructField('id', StringType(), True), \n",
    "          StructField('motivation', StringType(), True), \n",
    "          StructField('share', StringType(), True), \n",
    "          StructField('surname', StringType(), True)\n",
    "          ]), True), True),\n",
    "     StructField('overallMotivation', StringType(), True), \n",
    "     StructField('year', IntegerType(), True)])\n",
    "\n",
    "json_df_with_schema = (\n",
    "    spark.read.format(\"json\")\n",
    "    .schema(json_schema)\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"mode\", \"PERMISSIVE\")\n",
    "    .option(\"columnNameOfCorruptRecord\", \"corrupt_record\")\n",
    "    .load(\"/Volumes/mycatalog/myschema/myvolume/repofiles/Data-Engineering-with-Databricks-Cookbook-main/data/nobel_prizes.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d216757-b1c1-4668-8831-36ed4b1c31e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### `get_json_object()` and `json_tuple()` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a8ebef-452f-4a50-86d4-683395a5c912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# create a DataFrame with a JSON string column\n",
    "df = spark.createDataFrame([\n",
    "  (1, '{\"name\": \"Alice\", \"age\": 25}'),\n",
    "  (2, '{\"name\": \"Bob\", \"age\": 30}')\n",
    "], [\"id\", \"json_data\"])\n",
    "\n",
    "# extract the \"name\" field from the JSON string column\n",
    "name_df = df.select(get_json_object(\"json_data\", \"$.name\").alias(\"name\"))\n",
    "\n",
    "# cast the extracted value to a string\n",
    "name_str_df = name_df.withColumn(\"name_str\", name_df[\"name\"].cast(StringType()))\n",
    "\n",
    "name_str_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2843d1dc-c08f-42d9-94eb-84e75a15a099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "# create a DataFrame with a JSON string column\n",
    "df = spark.createDataFrame([\n",
    "  (1, '{\"name\": \"Alice\", \"age\": 25}'),\n",
    "  (2, '{\"name\": \"Bob\", \"age\": 30}')\n",
    "], [\"id\", \"json_data\"])\n",
    "\n",
    "# extract the \"name\" and \"age\" fields from the JSON string column\n",
    "name_age_df = df.select(json_tuple(\"json_data\", \"name\", \"age\").alias(\"name\", \"age\"))\n",
    "\n",
    "name_age_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a5b7f9-f33d-4159-87de-643aaea1db2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### `flatten()` and `collect_list()` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4983a730-3417-44b6-bb65-b4752141a46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import flatten, collect_list\n",
    "\n",
    "# create a DataFrame with an array of arrays column\n",
    "df = spark.createDataFrame([\n",
    "  (1, [[1, 2], [3, 4], [5, 6]]),\n",
    "  (2, [[7, 8], [9, 10], [11, 12]])\n",
    "], [\"id\", \"data\"])\n",
    "\n",
    "# use collect_list() function to group by specified columns\n",
    "collect_df = df.select(collect_list(\"data\").alias(\"data\"))\n",
    "collect_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e74cdf-3cea-43dd-8516-35bdb0c248e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use flatten() function to merge all the elements of the inner arrays\n",
    "flattened_df = collect_df.select(flatten(\"data\").alias(\"merged_data\"))\n",
    "flattened_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e335a3-6131-453c-8546-7f36f164a296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, flatten, collect_list\n",
    "\n",
    "# create a DataFrame with nested array column\n",
    "df = spark.createDataFrame([\n",
    "  (1, [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]),\n",
    "  (2, [[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n",
    "], [\"id\", \"data\"])\n",
    "\n",
    "# explode the outermost array to flatten the structure\n",
    "exploded_df = df.select(col(\"id\"),explode(\"data\").alias(\"inner_data\"))\n",
    "\n",
    "\n",
    "# # use collect_list() to group all the inner arrays together\n",
    "grouped_df = exploded_df.groupBy(\"id\").agg(collect_list(\"inner_data\").alias(\"merged_data\"))\n",
    "\n",
    "# # use flatten() to merge all the elements of the inner arrays\n",
    "flattened_df = grouped_df.select(flatten(\"merged_data\").alias(\"final_data\"))\n",
    "\n",
    "flattened_df.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.2 read-json-data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
