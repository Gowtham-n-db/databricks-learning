{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7da85893-a9c9-4437-9c1b-99a591d06b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Creating a dataframe and removing data skewness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9805445a-0d29-4b60-8327-0bc43eba694c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Transactions table (big table, skewed)\n",
    "Transactions = spark.createDataFrame([\n",
    "    (1, 101, \"R1\", 100),\n",
    "    (2, 101, \"R1\", 200),\n",
    "    (3, 101, \"R1\", 150),\n",
    "    (4, 102, \"R2\", 300),\n",
    "    (5, 103, \"R3\", 400),\n",
    "    (6, 101, \"R1\", 250),  # heavily skewed customer_id=101, region=R1\n",
    "], [\"txn_id\", \"customer_id\", \"region_id\", \"amount\"])\n",
    "\n",
    "# Customers table (small table)\n",
    "Customers = spark.createDataFrame([\n",
    "    (101, \"R1\", \"Ravi\", \"Hyderabad\"),\n",
    "    (102, \"R2\", \"Priya\", \"Mumbai\"),\n",
    "    (103, \"R3\", \"John\", \"Delhi\"),\n",
    "], [\"customer_id\", \"region_id\", \"name\", \"city\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2290ca3b-e630-4f29-b311-e23c27d716d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#normal join \n",
    "\n",
    "df_normal = Transactions.join(Customers, [\"customer_id\", \"region_id\"], \"inner\")\n",
    "df_normal.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a1971c-ee81-4294-bd6e-79c3cfb0901e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#broadcast\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_broadcast = Transactions.join(broadcast(Customers), [\"customer_id\", \"region_id\"], \"inner\")\n",
    "df_broadcast.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63130cc8-c7c5-434f-8369-b654817dd9ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, explode, lit, array\n",
    "\n",
    "# Step 4.1: Add salts to Transactions\n",
    "Transactions_salted = (\n",
    "    Transactions\n",
    "    .withColumn(\"cust_salt\", (rand()*3).cast(\"int\"))   # 3 salts for customer_id\n",
    "    .withColumn(\"region_salt\", (rand()*2).cast(\"int\")) # 2 salts for region_id\n",
    ")\n",
    "\n",
    "# Step 4.2: Replicate Customers table with same salts\n",
    "Customers_salted = (\n",
    "    Customers\n",
    "    .withColumn(\"cust_salt\", explode(array([lit(i) for i in range(3)])))\n",
    "    .withColumn(\"region_salt\", explode(array(lit(0), lit(1))))\n",
    ")\n",
    "\n",
    "# Step 4.3: Join with salts\n",
    "df_salted = Transactions_salted.join(\n",
    "    Customers_salted,\n",
    "    [\"customer_id\", \"region_id\", \"cust_salt\", \"region_salt\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "df_salted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd11208a-3e06-4d11-be03-a8471895dccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Normal Join Result:\")\n",
    "df_normal.show()\n",
    "\n",
    "print(\"Broadcast Join Result:\")\n",
    "df_broadcast.show()\n",
    "\n",
    "print(\"Salting Join Result (2 skewed keys):\")\n",
    "df_salted.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data skweness",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
