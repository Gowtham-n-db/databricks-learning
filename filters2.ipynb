{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43fbd73-174f-4c35-b5fe-a2d31c46f93f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, avg, sum, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.show(truncate=False)\n",
    "\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df1 =(df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    " .withColumn(\"Rank\", rank().over(windowSpec)) \\\n",
    " .withColumn(\"dense_rank\", dense_rank().over(windowSpec))\n",
    " )\n",
    "\n",
    "agg = (df.agg(\n",
    "      avg(\"salary\"). alias(\"avg_salary\"), \\\n",
    "      sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      count(\"salary\").alias(\"count_salary\")))\n",
    "\n",
    "avg_salary = agg.collect()[0][\"avg_salary\"]\n",
    "\n",
    "f = (\n",
    "  df1.filter(\n",
    "      (col(\"salary\") > avg_salary) |\n",
    "      (col(\"rank\") == 1) &\n",
    "      (col(\"dense_rank\") == 1)\n",
    "  )\n",
    "  .select(\"employee_name\", \"department\", \"salary\")\n",
    ")\n",
    "\n",
    "g = (f.orderBy\n",
    "     (col(\"salary\")\n",
    "      .desc())\n",
    ")\n",
    "\n",
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "096d96ea-1b79-49ee-a2f7-1b840eeae313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, sum, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number, lit\n",
    "\n",
    "simpleData = [\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "]\n",
    "\n",
    "columns = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema=columns)\n",
    "df.show(truncate=False)\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df1 = (\n",
    "    df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "      .withColumn(\"Rank\", rank().over(windowSpec))\n",
    "      .withColumn(\"dense_rank\", dense_rank().over(windowSpec)\n",
    "                  )\n",
    ")\n",
    "\n",
    "agg = df.agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    sum(\"salary\").alias(\"sum_salary\"),\n",
    "    count(\"salary\").alias(\"count_salary\")\n",
    ")\n",
    "\n",
    "avg_salary = agg.collect()[0][\"avg_salary\"]\n",
    "\n",
    "f = (\n",
    "    df1.filter(\n",
    "        (col(\"salary\") > avg_salary) &\n",
    "        ((col(\"Rank\") == 1) & (col(\"dense_rank\") == 1))\n",
    "    )\n",
    "    .withColumn(\"avg_salary\", lit(avg_salary))\n",
    "    .select(\"employee_name\", \"department\", \"salary\", \"avg_salary\", \"row_number\", \"Rank\", \"dense_rank\")\n",
    ")\n",
    "\n",
    "g = f.orderBy(col(\"salary\").desc())\n",
    "\n",
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd98dc4c-9bba-432f-8acc-78aabe8b6d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df1 =(df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    " .withColumn(\"Rank\", rank().over(windowSpec)) \\\n",
    " .withColumn(\"dense_rank\", dense_rank().over(windowSpec)))\n",
    "\n",
    "f = (df1.filter\n",
    "     ((col(\"row_number\") == 1) & \n",
    "      (col(\"rank\") == 1) &\n",
    "       (col(\"dense_rank\") == 1))\n",
    "        .select(\"employee_name\", \"department\", \"salary\"))\n",
    "display(f)\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289a96e4-c53a-49e9-91cd-e4b98a59b744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
    "    .show()\n",
    "    \n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()\n",
    "\n",
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()\n",
    "\n",
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()\n",
    "    \n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "filters2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
